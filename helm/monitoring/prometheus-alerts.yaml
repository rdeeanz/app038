groups:
  - name: slo.rules
    interval: 30s
    rules:
      # Availability SLO: 99.9% uptime
      - record: job:http_requests_total:rate5m
        expr: sum(rate(http_requests_total[5m])) by (job, instance)
      
      - record: job:http_requests_total:errors:rate5m
        expr: sum(rate(http_requests_total{status=~"5.."}[5m])) by (job, instance)
      
      - record: job:availability:ratio
        expr: |
          1 - (
            job:http_requests_total:errors:rate5m
            /
            job:http_requests_total:rate5m
          )
      
      # Latency SLO: 99th percentile < 500ms
      - record: job:http_request_duration_seconds:rate5m
        expr: sum(rate(http_request_duration_seconds_bucket[5m])) by (job, le)
      
      - record: job:http_request_duration_seconds:p99
        expr: histogram_quantile(0.99, job:http_request_duration_seconds:rate5m)
      
      # Error Rate SLO: < 0.1% errors
      - record: job:error_rate:ratio
        expr: |
          job:http_requests_total:errors:rate5m
          /
          job:http_requests_total:rate5m

  - name: slo.alerts
    interval: 30s
    rules:
      # Availability SLO Violation
      - alert: AvailabilitySLOViolation
        expr: |
          (
            job:availability:ratio < 0.999
          ) and (
            job:http_requests_total:rate5m > 0
          )
        for: 5m
        labels:
          severity: critical
          slo: availability
          team: platform
        annotations:
          summary: "Availability SLO violation for {{ $labels.job }}"
          description: |
            Availability is {{ $value | humanizePercentage }} for {{ $labels.job }}.
            Target: 99.9% (0.999)
            Current: {{ $value | humanizePercentage }}
            Duration: 5 minutes
          runbook_url: "https://runbooks.example.com/availability-slo"
          dashboard_url: "https://grafana.app038.local/d/slo-dashboard"
      
      # Latency SLO Violation
      - alert: LatencySLOViolation
        expr: |
          (
            job:http_request_duration_seconds:p99 > 0.5
          ) and (
            job:http_requests_total:rate5m > 0
          )
        for: 5m
        labels:
          severity: critical
          slo: latency
          team: platform
        annotations:
          summary: "Latency SLO violation for {{ $labels.job }}"
          description: |
            99th percentile latency is {{ $value | humanizeDuration }} for {{ $labels.job }}.
            Target: < 500ms (0.5s)
            Current: {{ $value | humanizeDuration }}
            Duration: 5 minutes
          runbook_url: "https://runbooks.example.com/latency-slo"
          dashboard_url: "https://grafana.app038.local/d/slo-dashboard"
      
      # Error Rate SLO Violation
      - alert: ErrorRateSLOViolation
        expr: |
          (
            job:error_rate:ratio > 0.001
          ) and (
            job:http_requests_total:rate5m > 0
          )
        for: 5m
        labels:
          severity: critical
          slo: error_rate
          team: platform
        annotations:
          summary: "Error rate SLO violation for {{ $labels.job }}"
          description: |
            Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}.
            Target: < 0.1% (0.001)
            Current: {{ $value | humanizePercentage }}
            Duration: 5 minutes
          runbook_url: "https://runbooks.example.com/error-rate-slo"
          dashboard_url: "https://grafana.app038.local/d/slo-dashboard"
      
      # Burn Rate Alert (30-day error budget)
      - alert: HighErrorBudgetBurnRate
        expr: |
          (
            job:error_rate:ratio > 0.002
          ) and (
            job:http_requests_total:rate5m > 0
          )
        for: 1m
        labels:
          severity: warning
          slo: error_budget
          team: platform
        annotations:
          summary: "High error budget burn rate for {{ $labels.job }}"
          description: |
            Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}.
            This is consuming error budget at 2x the target rate.
            If this continues, the 30-day error budget will be exhausted.
          runbook_url: "https://runbooks.example.com/error-budget"
          dashboard_url: "https://grafana.app038.local/d/slo-dashboard"

  - name: application.alerts
    interval: 30s
    rules:
      # High Request Rate
      - alert: HighRequestRate
        expr: |
          sum(rate(http_requests_total[5m])) by (job) > 1000
        for: 10m
        labels:
          severity: warning
          team: app
        annotations:
          summary: "High request rate for {{ $labels.job }}"
          description: |
            Request rate is {{ $value | humanize }} req/s for {{ $labels.job }}.
            This is above the normal threshold of 1000 req/s.
      
      # High Memory Usage
      - alert: HighMemoryUsage
        expr: |
          (
            container_memory_usage_bytes{container!="POD",container!=""}
            /
            container_spec_memory_limit_bytes
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage for {{ $labels.pod }}"
          description: |
            Memory usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}.
            Limit: {{ $labels.container }}
      
      # High CPU Usage
      - alert: HighCPUUsage
        expr: |
          (
            rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m])
            /
            container_spec_cpu_quota
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage for {{ $labels.pod }}"
          description: |
            CPU usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}.
            Container: {{ $labels.container }}
      
      # Pod CrashLooping
      - alert: PodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: |
            Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently.
            Restart rate: {{ $value | humanize }} restarts/min
      
      # Database Connection Pool Exhausted
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          database_connections_active / database_connections_max > 0.9
        for: 5m
        labels:
          severity: critical
          team: app
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: |
            Database connection pool usage is {{ $value | humanizePercentage }}.
            Active: {{ $labels.active }}
            Max: {{ $labels.max }}
      
      # Queue Backlog Growing
      - alert: QueueBacklogGrowing
        expr: |
          increase(queue_jobs_total[5m]) > 1000
        for: 10m
        labels:
          severity: warning
          team: app
        annotations:
          summary: "Queue backlog is growing for {{ $labels.queue }}"
          description: |
            Queue {{ $labels.queue }} has {{ $value | humanize }} pending jobs.
            This indicates processing is falling behind.

  - name: infrastructure.alerts
    interval: 30s
    rules:
      # Node Down
      - alert: NodeDown
        expr: up{job="node-exporter"} == 0
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: |
            Node {{ $labels.instance }} has been down for more than 5 minutes.
      
      # Disk Space Low
      - alert: DiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{fstype!="tmpfs"}
            /
            node_filesystem_size_bytes{fstype!="tmpfs"}
          ) < 0.1
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: |
            Disk {{ $labels.mountpoint }} on {{ $labels.instance }} has only {{ $value | humanizePercentage }} free space.
      
      # High Disk I/O
      - alert: HighDiskIO
        expr: |
          rate(node_disk_io_time_seconds_total[5m]) > 0.9
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High disk I/O on {{ $labels.instance }}"
          description: |
            Disk I/O utilization is {{ $value | humanizePercentage }} on {{ $labels.instance }}.
            Device: {{ $labels.device }}

